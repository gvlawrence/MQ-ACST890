{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Question 3</h1>\n",
    "<p>\n",
    "<h2>\n",
    "Part a\n",
    "</h2>\n",
    "</P>\n",
    "<p>\n",
    "Spreadsheet <code>student_results.csv</code> is in my directory <code>~/AnacondaProjects</code>. We read it in below.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID number</th>\n",
       "      <th>Assessment 1</th>\n",
       "      <th>Assessment 2</th>\n",
       "      <th>Online quiz 1</th>\n",
       "      <th>Online quiz 2</th>\n",
       "      <th>Online quiz 3</th>\n",
       "      <th>Online quiz 4</th>\n",
       "      <th>Group assignment 1</th>\n",
       "      <th>Group assignment 2</th>\n",
       "      <th>Assessment 3</th>\n",
       "      <th>Assessment 4</th>\n",
       "      <th>Group assignment 3</th>\n",
       "      <th>Group evaluation</th>\n",
       "      <th>Final exam</th>\n",
       "      <th>Class No</th>\n",
       "      <th>groupname</th>\n",
       "      <th>Student category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54002768</td>\n",
       "      <td>17.21</td>\n",
       "      <td>24.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>18.92</td>\n",
       "      <td>26.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>69</td>\n",
       "      <td>9</td>\n",
       "      <td>cl9tg2</td>\n",
       "      <td>domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54005658</td>\n",
       "      <td>66.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>18.14</td>\n",
       "      <td>29.00</td>\n",
       "      <td>19.91</td>\n",
       "      <td>22.35</td>\n",
       "      <td>17.04</td>\n",
       "      <td>60.54</td>\n",
       "      <td>24.07</td>\n",
       "      <td>42.5</td>\n",
       "      <td>1.88</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>cl2tg8</td>\n",
       "      <td>domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54010438</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>43.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>cl3tg7</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54011658</td>\n",
       "      <td>60.67</td>\n",
       "      <td>29.07</td>\n",
       "      <td>27.50</td>\n",
       "      <td>18.40</td>\n",
       "      <td>24.50</td>\n",
       "      <td>18.52</td>\n",
       "      <td>25.00</td>\n",
       "      <td>19.29</td>\n",
       "      <td>66.11</td>\n",
       "      <td>27.61</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.72</td>\n",
       "      <td>75</td>\n",
       "      <td>20</td>\n",
       "      <td>cl20tg2</td>\n",
       "      <td>domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54012789</td>\n",
       "      <td>37.33</td>\n",
       "      <td>29.01</td>\n",
       "      <td>11.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>19.50</td>\n",
       "      <td>19.10</td>\n",
       "      <td>27.75</td>\n",
       "      <td>17.82</td>\n",
       "      <td>55.61</td>\n",
       "      <td>19.21</td>\n",
       "      <td>42.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "      <td>cl19tg6</td>\n",
       "      <td>domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>54013792</td>\n",
       "      <td>47.44</td>\n",
       "      <td>27.23</td>\n",
       "      <td>15.00</td>\n",
       "      <td>18.71</td>\n",
       "      <td>19.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>48.22</td>\n",
       "      <td>27.76</td>\n",
       "      <td>47.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>45</td>\n",
       "      <td>21</td>\n",
       "      <td>cl21tg10</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>54018502</td>\n",
       "      <td>69.30</td>\n",
       "      <td>26.83</td>\n",
       "      <td>25.00</td>\n",
       "      <td>17.76</td>\n",
       "      <td>24.00</td>\n",
       "      <td>15.70</td>\n",
       "      <td>23.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>63.78</td>\n",
       "      <td>24.42</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.62</td>\n",
       "      <td>68</td>\n",
       "      <td>3</td>\n",
       "      <td>cl3tg6</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>54020391</td>\n",
       "      <td>66.89</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>19.50</td>\n",
       "      <td>28.00</td>\n",
       "      <td>18.15</td>\n",
       "      <td>29.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>66.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>35</td>\n",
       "      <td>21</td>\n",
       "      <td>cl21tg3</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>54020678</td>\n",
       "      <td>19.25</td>\n",
       "      <td>27.12</td>\n",
       "      <td>10.50</td>\n",
       "      <td>19.31</td>\n",
       "      <td>18.38</td>\n",
       "      <td>19.49</td>\n",
       "      <td>23.00</td>\n",
       "      <td>19.15</td>\n",
       "      <td>31.06</td>\n",
       "      <td>28.22</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>cl8tg5</td>\n",
       "      <td>domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>54030996</td>\n",
       "      <td>0.00</td>\n",
       "      <td>28.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.98</td>\n",
       "      <td>25.29</td>\n",
       "      <td>16.50</td>\n",
       "      <td>15.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>32.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>cl4tg7</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>54031400</td>\n",
       "      <td>64.56</td>\n",
       "      <td>27.68</td>\n",
       "      <td>27.19</td>\n",
       "      <td>19.99</td>\n",
       "      <td>18.00</td>\n",
       "      <td>18.75</td>\n",
       "      <td>28.13</td>\n",
       "      <td>17.21</td>\n",
       "      <td>61.06</td>\n",
       "      <td>26.46</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>78</td>\n",
       "      <td>7</td>\n",
       "      <td>cl7tg1</td>\n",
       "      <td>domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>54055936</td>\n",
       "      <td>70.00</td>\n",
       "      <td>29.42</td>\n",
       "      <td>17.00</td>\n",
       "      <td>19.40</td>\n",
       "      <td>20.50</td>\n",
       "      <td>17.51</td>\n",
       "      <td>27.75</td>\n",
       "      <td>18.78</td>\n",
       "      <td>50.60</td>\n",
       "      <td>16.13</td>\n",
       "      <td>22.5</td>\n",
       "      <td>1.80</td>\n",
       "      <td>49</td>\n",
       "      <td>11</td>\n",
       "      <td>cl11tg2</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>54057724</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>47.5</td>\n",
       "      <td>1.22</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>cl1tg6</td>\n",
       "      <td>domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>54060699</td>\n",
       "      <td>69.61</td>\n",
       "      <td>28.75</td>\n",
       "      <td>24.00</td>\n",
       "      <td>16.13</td>\n",
       "      <td>26.50</td>\n",
       "      <td>18.79</td>\n",
       "      <td>25.88</td>\n",
       "      <td>17.07</td>\n",
       "      <td>52.89</td>\n",
       "      <td>27.23</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1.78</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>cl2tg2</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>54061544</td>\n",
       "      <td>42.78</td>\n",
       "      <td>28.67</td>\n",
       "      <td>20.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>37.5</td>\n",
       "      <td>0.80</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>cl2tg3</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>54065096</td>\n",
       "      <td>67.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>26.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>14.87</td>\n",
       "      <td>24.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>64.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.24</td>\n",
       "      <td>41</td>\n",
       "      <td>21</td>\n",
       "      <td>cl21tg4</td>\n",
       "      <td>domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>54067662</td>\n",
       "      <td>49.58</td>\n",
       "      <td>27.96</td>\n",
       "      <td>21.00</td>\n",
       "      <td>18.20</td>\n",
       "      <td>25.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>28.50</td>\n",
       "      <td>16.76</td>\n",
       "      <td>47.40</td>\n",
       "      <td>29.45</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.84</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>cl16tg4</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>54068014</td>\n",
       "      <td>68.60</td>\n",
       "      <td>29.19</td>\n",
       "      <td>19.13</td>\n",
       "      <td>17.73</td>\n",
       "      <td>28.00</td>\n",
       "      <td>18.69</td>\n",
       "      <td>28.20</td>\n",
       "      <td>18.36</td>\n",
       "      <td>10.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>40</td>\n",
       "      <td>12</td>\n",
       "      <td>cl12tg7</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>54069136</td>\n",
       "      <td>51.04</td>\n",
       "      <td>29.84</td>\n",
       "      <td>15.00</td>\n",
       "      <td>13.68</td>\n",
       "      <td>27.00</td>\n",
       "      <td>18.80</td>\n",
       "      <td>22.50</td>\n",
       "      <td>18.05</td>\n",
       "      <td>61.54</td>\n",
       "      <td>29.15</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>45</td>\n",
       "      <td>8</td>\n",
       "      <td>cl8tg7</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>54071403</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.22</td>\n",
       "      <td>22.5</td>\n",
       "      <td>0.64</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>cl10tg8</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>54076167</td>\n",
       "      <td>0.00</td>\n",
       "      <td>28.81</td>\n",
       "      <td>22.00</td>\n",
       "      <td>19.28</td>\n",
       "      <td>28.00</td>\n",
       "      <td>17.95</td>\n",
       "      <td>18.86</td>\n",
       "      <td>17.64</td>\n",
       "      <td>60.28</td>\n",
       "      <td>25.60</td>\n",
       "      <td>22.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "      <td>cl3tg7</td>\n",
       "      <td>domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>54077942</td>\n",
       "      <td>68.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>18.53</td>\n",
       "      <td>27.00</td>\n",
       "      <td>17.83</td>\n",
       "      <td>29.14</td>\n",
       "      <td>17.28</td>\n",
       "      <td>61.25</td>\n",
       "      <td>25.88</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>55</td>\n",
       "      <td>16</td>\n",
       "      <td>cl16tg2</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>54079287</td>\n",
       "      <td>27.13</td>\n",
       "      <td>29.25</td>\n",
       "      <td>16.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.50</td>\n",
       "      <td>19.40</td>\n",
       "      <td>22.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.93</td>\n",
       "      <td>25.65</td>\n",
       "      <td>12.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "      <td>cl3tg4</td>\n",
       "      <td>domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>54082154</td>\n",
       "      <td>69.42</td>\n",
       "      <td>26.57</td>\n",
       "      <td>13.00</td>\n",
       "      <td>18.35</td>\n",
       "      <td>29.50</td>\n",
       "      <td>19.90</td>\n",
       "      <td>18.83</td>\n",
       "      <td>19.73</td>\n",
       "      <td>67.67</td>\n",
       "      <td>25.00</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>cl2tg10</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>54085768</td>\n",
       "      <td>70.00</td>\n",
       "      <td>28.46</td>\n",
       "      <td>28.50</td>\n",
       "      <td>18.33</td>\n",
       "      <td>24.00</td>\n",
       "      <td>19.80</td>\n",
       "      <td>21.64</td>\n",
       "      <td>19.46</td>\n",
       "      <td>58.72</td>\n",
       "      <td>27.68</td>\n",
       "      <td>32.5</td>\n",
       "      <td>0.80</td>\n",
       "      <td>45</td>\n",
       "      <td>17</td>\n",
       "      <td>cl17tg3</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>54088138</td>\n",
       "      <td>63.78</td>\n",
       "      <td>29.25</td>\n",
       "      <td>28.13</td>\n",
       "      <td>18.34</td>\n",
       "      <td>23.50</td>\n",
       "      <td>19.06</td>\n",
       "      <td>28.17</td>\n",
       "      <td>17.11</td>\n",
       "      <td>62.22</td>\n",
       "      <td>25.95</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.84</td>\n",
       "      <td>74</td>\n",
       "      <td>15</td>\n",
       "      <td>cl15tg10</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>54089161</td>\n",
       "      <td>68.54</td>\n",
       "      <td>30.00</td>\n",
       "      <td>16.50</td>\n",
       "      <td>18.18</td>\n",
       "      <td>28.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23.25</td>\n",
       "      <td>16.51</td>\n",
       "      <td>60.28</td>\n",
       "      <td>14.49</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>50</td>\n",
       "      <td>19</td>\n",
       "      <td>cl19tg3</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>54092340</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.45</td>\n",
       "      <td>19.35</td>\n",
       "      <td>29.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>28.13</td>\n",
       "      <td>12.35</td>\n",
       "      <td>62.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>cl10tg4</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>54098547</td>\n",
       "      <td>53.08</td>\n",
       "      <td>29.75</td>\n",
       "      <td>11.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>39.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>cl5tg5</td>\n",
       "      <td>domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>54098744</td>\n",
       "      <td>49.00</td>\n",
       "      <td>27.76</td>\n",
       "      <td>12.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.75</td>\n",
       "      <td>18.97</td>\n",
       "      <td>18.00</td>\n",
       "      <td>18.08</td>\n",
       "      <td>52.21</td>\n",
       "      <td>29.33</td>\n",
       "      <td>22.5</td>\n",
       "      <td>1.74</td>\n",
       "      <td>61</td>\n",
       "      <td>9</td>\n",
       "      <td>cl9tg1</td>\n",
       "      <td>domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>55900350</td>\n",
       "      <td>61.54</td>\n",
       "      <td>29.98</td>\n",
       "      <td>20.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.50</td>\n",
       "      <td>15.44</td>\n",
       "      <td>26.33</td>\n",
       "      <td>18.32</td>\n",
       "      <td>52.79</td>\n",
       "      <td>23.00</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>33</td>\n",
       "      <td>10</td>\n",
       "      <td>cl10tg1</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>55901281</td>\n",
       "      <td>8.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>14.80</td>\n",
       "      <td>62.22</td>\n",
       "      <td>23.65</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>43</td>\n",
       "      <td>17</td>\n",
       "      <td>cl17tg4</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>55902573</td>\n",
       "      <td>30.45</td>\n",
       "      <td>28.25</td>\n",
       "      <td>16.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>14.85</td>\n",
       "      <td>19.70</td>\n",
       "      <td>15.33</td>\n",
       "      <td>14.44</td>\n",
       "      <td>43.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.46</td>\n",
       "      <td>46</td>\n",
       "      <td>5</td>\n",
       "      <td>cl5tg9</td>\n",
       "      <td>domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>55905403</td>\n",
       "      <td>34.42</td>\n",
       "      <td>25.96</td>\n",
       "      <td>16.50</td>\n",
       "      <td>19.58</td>\n",
       "      <td>23.50</td>\n",
       "      <td>19.34</td>\n",
       "      <td>24.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.85</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.76</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>cl11tg7</td>\n",
       "      <td>domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>55912788</td>\n",
       "      <td>51.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>19.38</td>\n",
       "      <td>13.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>38.27</td>\n",
       "      <td>24.87</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.52</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>cl2tg4</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>55913369</td>\n",
       "      <td>55.22</td>\n",
       "      <td>27.38</td>\n",
       "      <td>18.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.60</td>\n",
       "      <td>18.48</td>\n",
       "      <td>56.00</td>\n",
       "      <td>28.92</td>\n",
       "      <td>47.5</td>\n",
       "      <td>1.92</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>cl1tg6</td>\n",
       "      <td>domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>55916935</td>\n",
       "      <td>64.40</td>\n",
       "      <td>27.91</td>\n",
       "      <td>30.00</td>\n",
       "      <td>16.19</td>\n",
       "      <td>28.00</td>\n",
       "      <td>18.64</td>\n",
       "      <td>24.00</td>\n",
       "      <td>18.38</td>\n",
       "      <td>59.94</td>\n",
       "      <td>27.70</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.90</td>\n",
       "      <td>83</td>\n",
       "      <td>11</td>\n",
       "      <td>cl11tg7</td>\n",
       "      <td>domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>55917993</td>\n",
       "      <td>33.83</td>\n",
       "      <td>26.67</td>\n",
       "      <td>10.50</td>\n",
       "      <td>19.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.47</td>\n",
       "      <td>19.00</td>\n",
       "      <td>18.37</td>\n",
       "      <td>64.94</td>\n",
       "      <td>27.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>43</td>\n",
       "      <td>12</td>\n",
       "      <td>cl12tg6</td>\n",
       "      <td>domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>55920201</td>\n",
       "      <td>38.38</td>\n",
       "      <td>27.45</td>\n",
       "      <td>23.00</td>\n",
       "      <td>19.98</td>\n",
       "      <td>19.50</td>\n",
       "      <td>19.80</td>\n",
       "      <td>24.60</td>\n",
       "      <td>14.07</td>\n",
       "      <td>34.85</td>\n",
       "      <td>26.06</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.74</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>cl6tg9</td>\n",
       "      <td>domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>55921463</td>\n",
       "      <td>52.21</td>\n",
       "      <td>27.12</td>\n",
       "      <td>17.00</td>\n",
       "      <td>18.56</td>\n",
       "      <td>12.00</td>\n",
       "      <td>17.88</td>\n",
       "      <td>21.50</td>\n",
       "      <td>19.35</td>\n",
       "      <td>52.27</td>\n",
       "      <td>18.07</td>\n",
       "      <td>37.5</td>\n",
       "      <td>1.84</td>\n",
       "      <td>40</td>\n",
       "      <td>19</td>\n",
       "      <td>cl19tg1</td>\n",
       "      <td>domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>55923594</td>\n",
       "      <td>12.44</td>\n",
       "      <td>26.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>cl12tg4</td>\n",
       "      <td>domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>55927206</td>\n",
       "      <td>22.46</td>\n",
       "      <td>28.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.67</td>\n",
       "      <td>23.00</td>\n",
       "      <td>19.40</td>\n",
       "      <td>23.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>26.54</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.76</td>\n",
       "      <td>51</td>\n",
       "      <td>19</td>\n",
       "      <td>cl19tg3</td>\n",
       "      <td>domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>55928520</td>\n",
       "      <td>57.17</td>\n",
       "      <td>28.43</td>\n",
       "      <td>23.00</td>\n",
       "      <td>18.69</td>\n",
       "      <td>22.00</td>\n",
       "      <td>18.40</td>\n",
       "      <td>28.80</td>\n",
       "      <td>18.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>cl16tg3</td>\n",
       "      <td>domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>55932906</td>\n",
       "      <td>70.00</td>\n",
       "      <td>28.67</td>\n",
       "      <td>28.50</td>\n",
       "      <td>19.70</td>\n",
       "      <td>29.00</td>\n",
       "      <td>19.45</td>\n",
       "      <td>23.63</td>\n",
       "      <td>15.22</td>\n",
       "      <td>61.83</td>\n",
       "      <td>27.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "      <td>cl8tg6</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>55933053</td>\n",
       "      <td>62.85</td>\n",
       "      <td>29.86</td>\n",
       "      <td>24.50</td>\n",
       "      <td>17.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.96</td>\n",
       "      <td>25.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>65.33</td>\n",
       "      <td>28.75</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.62</td>\n",
       "      <td>61</td>\n",
       "      <td>20</td>\n",
       "      <td>cl20tg2</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>55937364</td>\n",
       "      <td>61.69</td>\n",
       "      <td>27.02</td>\n",
       "      <td>12.50</td>\n",
       "      <td>19.99</td>\n",
       "      <td>28.00</td>\n",
       "      <td>18.80</td>\n",
       "      <td>22.50</td>\n",
       "      <td>13.11</td>\n",
       "      <td>64.75</td>\n",
       "      <td>24.39</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>71</td>\n",
       "      <td>8</td>\n",
       "      <td>cl8tg7</td>\n",
       "      <td>domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>55946111</td>\n",
       "      <td>62.22</td>\n",
       "      <td>26.83</td>\n",
       "      <td>26.63</td>\n",
       "      <td>18.41</td>\n",
       "      <td>29.00</td>\n",
       "      <td>16.34</td>\n",
       "      <td>26.40</td>\n",
       "      <td>19.52</td>\n",
       "      <td>49.15</td>\n",
       "      <td>27.05</td>\n",
       "      <td>27.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>46</td>\n",
       "      <td>15</td>\n",
       "      <td>cl15tg7</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>55948388</td>\n",
       "      <td>36.02</td>\n",
       "      <td>28.64</td>\n",
       "      <td>14.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>26.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.00</td>\n",
       "      <td>17.69</td>\n",
       "      <td>28.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42.5</td>\n",
       "      <td>1.25</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>cl11tg9</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>55950070</td>\n",
       "      <td>66.50</td>\n",
       "      <td>29.51</td>\n",
       "      <td>13.50</td>\n",
       "      <td>15.73</td>\n",
       "      <td>28.50</td>\n",
       "      <td>19.40</td>\n",
       "      <td>23.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>53.20</td>\n",
       "      <td>29.08</td>\n",
       "      <td>42.5</td>\n",
       "      <td>1.80</td>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>cl4tg8</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>55950963</td>\n",
       "      <td>25.90</td>\n",
       "      <td>27.58</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.13</td>\n",
       "      <td>19.05</td>\n",
       "      <td>28.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>49.58</td>\n",
       "      <td>23.60</td>\n",
       "      <td>42.5</td>\n",
       "      <td>1.92</td>\n",
       "      <td>60</td>\n",
       "      <td>11</td>\n",
       "      <td>cl11tg8</td>\n",
       "      <td>domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>55952559</td>\n",
       "      <td>27.07</td>\n",
       "      <td>26.21</td>\n",
       "      <td>18.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>18.80</td>\n",
       "      <td>27.50</td>\n",
       "      <td>17.85</td>\n",
       "      <td>41.61</td>\n",
       "      <td>24.39</td>\n",
       "      <td>17.5</td>\n",
       "      <td>1.32</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>cl7tg5</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>55960134</td>\n",
       "      <td>68.25</td>\n",
       "      <td>26.30</td>\n",
       "      <td>17.63</td>\n",
       "      <td>15.95</td>\n",
       "      <td>29.50</td>\n",
       "      <td>20.00</td>\n",
       "      <td>29.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>58.72</td>\n",
       "      <td>27.39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>48</td>\n",
       "      <td>18</td>\n",
       "      <td>cl18tg3</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>55961633</td>\n",
       "      <td>21.88</td>\n",
       "      <td>25.40</td>\n",
       "      <td>7.50</td>\n",
       "      <td>14.21</td>\n",
       "      <td>20.63</td>\n",
       "      <td>14.91</td>\n",
       "      <td>25.00</td>\n",
       "      <td>15.83</td>\n",
       "      <td>52.50</td>\n",
       "      <td>25.44</td>\n",
       "      <td>47.5</td>\n",
       "      <td>1.72</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>cl5tg8</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>55964288</td>\n",
       "      <td>62.27</td>\n",
       "      <td>29.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.28</td>\n",
       "      <td>15.00</td>\n",
       "      <td>19.97</td>\n",
       "      <td>16.80</td>\n",
       "      <td>19.72</td>\n",
       "      <td>34.42</td>\n",
       "      <td>26.79</td>\n",
       "      <td>17.5</td>\n",
       "      <td>1.18</td>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>cl6tg11</td>\n",
       "      <td>domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>55969705</td>\n",
       "      <td>63.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>19.50</td>\n",
       "      <td>26.25</td>\n",
       "      <td>19.30</td>\n",
       "      <td>25.88</td>\n",
       "      <td>18.15</td>\n",
       "      <td>60.96</td>\n",
       "      <td>25.41</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1.82</td>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>cl2tg2</td>\n",
       "      <td>domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>55971994</td>\n",
       "      <td>47.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>61</td>\n",
       "      <td>21</td>\n",
       "      <td>cl21tg4</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>55973433</td>\n",
       "      <td>67.38</td>\n",
       "      <td>28.34</td>\n",
       "      <td>18.50</td>\n",
       "      <td>10.42</td>\n",
       "      <td>21.75</td>\n",
       "      <td>12.70</td>\n",
       "      <td>19.29</td>\n",
       "      <td>18.01</td>\n",
       "      <td>59.11</td>\n",
       "      <td>17.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>cl2tg9</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>55975964</td>\n",
       "      <td>0.00</td>\n",
       "      <td>26.84</td>\n",
       "      <td>19.50</td>\n",
       "      <td>18.75</td>\n",
       "      <td>6.50</td>\n",
       "      <td>17.39</td>\n",
       "      <td>28.00</td>\n",
       "      <td>17.65</td>\n",
       "      <td>48.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>cl7tg8</td>\n",
       "      <td>domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>55981165</td>\n",
       "      <td>28.35</td>\n",
       "      <td>29.35</td>\n",
       "      <td>14.00</td>\n",
       "      <td>19.22</td>\n",
       "      <td>14.10</td>\n",
       "      <td>19.90</td>\n",
       "      <td>15.33</td>\n",
       "      <td>20.00</td>\n",
       "      <td>43.17</td>\n",
       "      <td>27.39</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.76</td>\n",
       "      <td>46</td>\n",
       "      <td>5</td>\n",
       "      <td>cl5tg9</td>\n",
       "      <td>domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>55999975</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.16</td>\n",
       "      <td>13.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.40</td>\n",
       "      <td>11.54</td>\n",
       "      <td>25.29</td>\n",
       "      <td>18.90</td>\n",
       "      <td>32.67</td>\n",
       "      <td>12.89</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.88</td>\n",
       "      <td>48</td>\n",
       "      <td>21</td>\n",
       "      <td>cl21tg1</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>578 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID number  Assessment 1  Assessment 2  Online quiz 1   Online quiz 2  \\\n",
       "0     54002768         17.21         24.19            0.00           0.00   \n",
       "1     54005658         66.50          0.00           22.00          18.14   \n",
       "2     54010438          0.00          0.00            0.00           0.00   \n",
       "3     54011658         60.67         29.07           27.50          18.40   \n",
       "4     54012789         37.33         29.01           11.00          20.00   \n",
       "5     54013792         47.44         27.23           15.00          18.71   \n",
       "6     54018502         69.30         26.83           25.00          17.76   \n",
       "7     54020391         66.89          0.00           24.00          19.50   \n",
       "8     54020678         19.25         27.12           10.50          19.31   \n",
       "9     54030996          0.00         28.17            0.00           0.00   \n",
       "10    54031400         64.56         27.68           27.19          19.99   \n",
       "11    54055936         70.00         29.42           17.00          19.40   \n",
       "12    54057724          0.00          0.00            0.00           0.00   \n",
       "13    54060699         69.61         28.75           24.00          16.13   \n",
       "14    54061544         42.78         28.67           20.63           0.00   \n",
       "15    54065096         67.67          0.00           26.63           0.00   \n",
       "16    54067662         49.58         27.96           21.00          18.20   \n",
       "17    54068014         68.60         29.19           19.13          17.73   \n",
       "18    54069136         51.04         29.84           15.00          13.68   \n",
       "19    54071403          0.00         21.86            0.00           0.00   \n",
       "20    54076167          0.00         28.81           22.00          19.28   \n",
       "21    54077942         68.44          0.00           18.00          18.53   \n",
       "22    54079287         27.13         29.25           16.00           0.00   \n",
       "23    54082154         69.42         26.57           13.00          18.35   \n",
       "24    54085768         70.00         28.46           28.50          18.33   \n",
       "25    54088138         63.78         29.25           28.13          18.34   \n",
       "26    54089161         68.54         30.00           16.50          18.18   \n",
       "27    54092340          0.00          0.00           27.45          19.35   \n",
       "28    54098547         53.08         29.75           11.50           0.00   \n",
       "29    54098744         49.00         27.76           12.50           0.00   \n",
       "..         ...           ...           ...             ...            ...   \n",
       "548   55900350         61.54         29.98           20.00           0.00   \n",
       "549   55901281          8.17          0.00            4.50           0.00   \n",
       "550   55902573         30.45         28.25           16.00          20.00   \n",
       "551   55905403         34.42         25.96           16.50          19.58   \n",
       "552   55912788         51.80          0.00           10.00          19.38   \n",
       "553   55913369         55.22         27.38           18.50           0.00   \n",
       "554   55916935         64.40         27.91           30.00          16.19   \n",
       "555   55917993         33.83         26.67           10.50          19.54   \n",
       "556   55920201         38.38         27.45           23.00          19.98   \n",
       "557   55921463         52.21         27.12           17.00          18.56   \n",
       "558   55923594         12.44         26.63            0.00           0.00   \n",
       "559   55927206         22.46         28.12            0.00          18.67   \n",
       "560   55928520         57.17         28.43           23.00          18.69   \n",
       "561   55932906         70.00         28.67           28.50          19.70   \n",
       "562   55933053         62.85         29.86           24.50          17.31   \n",
       "563   55937364         61.69         27.02           12.50          19.99   \n",
       "564   55946111         62.22         26.83           26.63          18.41   \n",
       "565   55948388         36.02         28.64           14.50           0.00   \n",
       "566   55950070         66.50         29.51           13.50          15.73   \n",
       "567   55950963         25.90         27.58            8.50           0.00   \n",
       "568   55952559         27.07         26.21           18.50           0.00   \n",
       "569   55960134         68.25         26.30           17.63          15.95   \n",
       "570   55961633         21.88         25.40            7.50          14.21   \n",
       "571   55964288         62.27         29.17            0.00          19.28   \n",
       "572   55969705         63.00         30.00           19.00          19.50   \n",
       "573   55971994         47.06          0.00            0.00           0.00   \n",
       "574   55973433         67.38         28.34           18.50          10.42   \n",
       "575   55975964          0.00         26.84           19.50          18.75   \n",
       "576   55981165         28.35         29.35           14.00          19.22   \n",
       "577   55999975          0.00         15.16           13.00           0.00   \n",
       "\n",
       "     Online quiz 3  Online quiz 4  Group assignment 1  Group assignment 2  \\\n",
       "0            15.00          18.92               26.57                0.00   \n",
       "1            29.00          19.91               22.35               17.04   \n",
       "2             4.13           0.00               18.86                0.00   \n",
       "3            24.50          18.52               25.00               19.29   \n",
       "4            19.50          19.10               27.75               17.82   \n",
       "5            19.88           0.00               27.00                0.00   \n",
       "6            24.00          15.70               23.57                0.00   \n",
       "7            28.00          18.15               29.00                0.00   \n",
       "8            18.38          19.49               23.00               19.15   \n",
       "9             0.00          15.98               25.29               16.50   \n",
       "10           18.00          18.75               28.13               17.21   \n",
       "11           20.50          17.51               27.75               18.78   \n",
       "12            0.00           0.00               24.60                0.00   \n",
       "13           26.50          18.79               25.88               17.07   \n",
       "14            0.00           0.00               27.00                0.00   \n",
       "15           13.00          14.87               24.38                0.00   \n",
       "16           25.00          20.00               28.50               16.76   \n",
       "17           28.00          18.69               28.20               18.36   \n",
       "18           27.00          18.80               22.50               18.05   \n",
       "19            0.00           0.00               24.00                0.00   \n",
       "20           28.00          17.95               18.86               17.64   \n",
       "21           27.00          17.83               29.14               17.28   \n",
       "22            7.50          19.40               22.71                0.00   \n",
       "23           29.50          19.90               18.83               19.73   \n",
       "24           24.00          19.80               21.64               19.46   \n",
       "25           23.50          19.06               28.17               17.11   \n",
       "26           28.50           0.00               23.25               16.51   \n",
       "27           29.00           0.00               28.13               12.35   \n",
       "28            0.00           0.00               20.79                0.00   \n",
       "29           21.75          18.97               18.00               18.08   \n",
       "..             ...            ...                 ...                 ...   \n",
       "548          14.50          15.44               26.33               18.32   \n",
       "549          27.00           0.00               29.00               14.80   \n",
       "550          14.85          19.70               15.33               14.44   \n",
       "551          23.50          19.34               24.00                0.00   \n",
       "552          13.50           0.00               22.00                0.00   \n",
       "553           7.00           0.00               24.60               18.48   \n",
       "554          28.00          18.64               24.00               18.38   \n",
       "555           0.00          19.47               19.00               18.37   \n",
       "556          19.50          19.80               24.60               14.07   \n",
       "557          12.00          17.88               21.50               19.35   \n",
       "558           0.00           0.00               23.25                0.00   \n",
       "559          23.00          19.40               23.25                0.00   \n",
       "560          22.00          18.40               28.80               18.67   \n",
       "561          29.00          19.45               23.63               15.22   \n",
       "562           0.00          18.96               25.00                0.00   \n",
       "563          28.00          18.80               22.50               13.11   \n",
       "564          29.00          16.34               26.40               19.52   \n",
       "565          26.63           0.00               27.00               17.69   \n",
       "566          28.50          19.40               23.57                0.00   \n",
       "567           7.13          19.05               28.00                0.00   \n",
       "568           9.00          18.80               27.50               17.85   \n",
       "569          29.50          20.00               29.25                0.00   \n",
       "570          20.63          14.91               25.00               15.83   \n",
       "571          15.00          19.97               16.80               19.72   \n",
       "572          26.25          19.30               25.88               18.15   \n",
       "573           0.00           0.00               24.38                0.00   \n",
       "574          21.75          12.70               19.29               18.01   \n",
       "575           6.50          17.39               28.00               17.65   \n",
       "576          14.10          19.90               15.33               20.00   \n",
       "577           8.40          11.54               25.29               18.90   \n",
       "\n",
       "     Assessment 3  Assessment 4  Group assignment 3   Group evaluation  \\\n",
       "0            0.00          0.00                50.0               2.00   \n",
       "1           60.54         24.07                42.5               1.88   \n",
       "2           43.75          0.00                22.5               0.00   \n",
       "3           66.11         27.61                35.0               1.72   \n",
       "4           55.61         19.21                42.5               2.00   \n",
       "5           48.22         27.76                47.5               0.00   \n",
       "6           63.78         24.42                50.0               1.62   \n",
       "7           66.35          0.00                30.0               0.94   \n",
       "8           31.06         28.22                10.0               2.00   \n",
       "9           15.56          0.00                32.5               2.00   \n",
       "10          61.06         26.46                40.0               0.00   \n",
       "11          50.60         16.13                22.5               1.80   \n",
       "12          18.67          0.00                47.5               1.22   \n",
       "13          52.89         27.23                45.0               1.78   \n",
       "14           0.00          0.00                37.5               0.80   \n",
       "15          64.50          0.00                40.0               1.24   \n",
       "16          47.40         29.45                40.0               1.84   \n",
       "17          10.50          0.00                22.5               2.00   \n",
       "18          61.54         29.15                25.0               2.00   \n",
       "19           0.00         25.22                22.5               0.64   \n",
       "20          60.28         25.60                22.5               2.00   \n",
       "21          61.25         25.88                45.0               2.00   \n",
       "22          21.93         25.65                12.5               2.00   \n",
       "23          67.67         25.00                40.0               2.00   \n",
       "24          58.72         27.68                32.5               0.80   \n",
       "25          62.22         25.95                40.0               1.84   \n",
       "26          60.28         14.49                15.0               2.00   \n",
       "27          62.56          0.00                45.0               1.80   \n",
       "28          39.67          0.00                 0.0               0.00   \n",
       "29          52.21         29.33                22.5               1.74   \n",
       "..            ...           ...                 ...                ...   \n",
       "548         52.79         23.00                35.0               0.00   \n",
       "549         62.22         23.65                30.0               0.00   \n",
       "550         43.94          0.00                20.0               1.46   \n",
       "551          0.00         25.85                50.0               1.76   \n",
       "552         38.27         24.87                10.0               1.52   \n",
       "553         56.00         28.92                47.5               1.92   \n",
       "554         59.94         27.70                50.0               1.90   \n",
       "555         64.94         27.08                 0.0               0.00   \n",
       "556         34.85         26.06                35.0               1.74   \n",
       "557         52.27         18.07                37.5               1.84   \n",
       "558          0.00          0.00                25.0               0.00   \n",
       "559          0.00         26.54                15.0               1.76   \n",
       "560          0.00         24.96                 0.0               0.00   \n",
       "561         61.83         27.80                 0.0               2.00   \n",
       "562         65.33         28.75                35.0               1.62   \n",
       "563         64.75         24.39                25.0               1.20   \n",
       "564         49.15         27.05                27.5               2.00   \n",
       "565         28.00          0.00                42.5               1.25   \n",
       "566         53.20         29.08                42.5               1.80   \n",
       "567         49.58         23.60                42.5               1.92   \n",
       "568         41.61         24.39                17.5               1.32   \n",
       "569         58.72         27.39                 0.0               2.00   \n",
       "570         52.50         25.44                47.5               1.72   \n",
       "571         34.42         26.79                17.5               1.18   \n",
       "572         60.96         25.41                45.0               1.82   \n",
       "573         40.00          0.00                40.0               0.00   \n",
       "574         59.11         17.15                 0.0               0.00   \n",
       "575         48.61          0.00                45.0               2.00   \n",
       "576         43.17         27.39                20.0               1.76   \n",
       "577         32.67         12.89                15.0               1.88   \n",
       "\n",
       "     Final exam  Class No groupname Student category  \n",
       "0            69         9    cl9tg2         domestic  \n",
       "1            63         2    cl2tg8         domestic  \n",
       "2            15         3    cl3tg7    international  \n",
       "3            75        20   cl20tg2         domestic  \n",
       "4            33        19   cl19tg6         domestic  \n",
       "5            45        21  cl21tg10    international  \n",
       "6            68         3    cl3tg6    international  \n",
       "7            35        21   cl21tg3    international  \n",
       "8            33         8    cl8tg5         domestic  \n",
       "9            10         4    cl4tg7    international  \n",
       "10           78         7    cl7tg1         domestic  \n",
       "11           49        11   cl11tg2    international  \n",
       "12           10         1    cl1tg6         domestic  \n",
       "13           46         2    cl2tg2    international  \n",
       "14           74         2    cl2tg3    international  \n",
       "15           41        21   cl21tg4         domestic  \n",
       "16           64        16   cl16tg4    international  \n",
       "17           40        12   cl12tg7    international  \n",
       "18           45         8    cl8tg7    international  \n",
       "19           10        10   cl10tg8    international  \n",
       "20           47         3    cl3tg7         domestic  \n",
       "21           55        16   cl16tg2    international  \n",
       "22           47         3    cl3tg4         domestic  \n",
       "23           52         2   cl2tg10    international  \n",
       "24           45        17   cl17tg3    international  \n",
       "25           74        15  cl15tg10    international  \n",
       "26           50        19   cl19tg3    international  \n",
       "27           34        10   cl10tg4    international  \n",
       "28           10         5    cl5tg5         domestic  \n",
       "29           61         9    cl9tg1         domestic  \n",
       "..          ...       ...       ...              ...  \n",
       "548          33        10   cl10tg1    international  \n",
       "549          43        17   cl17tg4    international  \n",
       "550          46         5    cl5tg9         domestic  \n",
       "551          10        11   cl11tg7         domestic  \n",
       "552          41         2    cl2tg4    international  \n",
       "553          66         1    cl1tg6         domestic  \n",
       "554          83        11   cl11tg7         domestic  \n",
       "555          43        12   cl12tg6         domestic  \n",
       "556          52         6    cl6tg9         domestic  \n",
       "557          40        19   cl19tg1         domestic  \n",
       "558          10        12   cl12tg4         domestic  \n",
       "559          51        19   cl19tg3         domestic  \n",
       "560          64        16   cl16tg3         domestic  \n",
       "561          47         8    cl8tg6    international  \n",
       "562          61        20   cl20tg2    international  \n",
       "563          71         8    cl8tg7         domestic  \n",
       "564          46        15   cl15tg7    international  \n",
       "565          10        11   cl11tg9    international  \n",
       "566          47         4    cl4tg8    international  \n",
       "567          60        11   cl11tg8         domestic  \n",
       "568          10         7    cl7tg5    international  \n",
       "569          48        18   cl18tg3    international  \n",
       "570          10         5    cl5tg8    international  \n",
       "571          80         6   cl6tg11         domestic  \n",
       "572          90         2    cl2tg2         domestic  \n",
       "573          61        21   cl21tg4    international  \n",
       "574          55         2    cl2tg9    international  \n",
       "575          33         7    cl7tg8         domestic  \n",
       "576          46         5    cl5tg9         domestic  \n",
       "577          48        21   cl21tg1    international  \n",
       "\n",
       "[578 rows x 17 columns]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('student_results.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part b</h2>\n",
    "<p>\n",
    "Replace <code>NA</code> data fields with zeroes.\n",
    "</p>\n",
    "<p>\n",
    "There are a variety of different ways to do this&mdash;as in internet search will reveal, for example\n",
    "<a href=\"https://stackoverflow.com/questions/13295735/how-can-i-replace-all-the-nan-values-with-zeros-in-a-column-of-a-pandas-datafra/30587837\">here</a>. I've used the Python string <code>replace</code> method, but other approaches are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    False\n",
      "1    False\n",
      "2    False\n",
      "3    False\n",
      "4    False\n",
      "Name: Assessment 1, dtype: bool\n",
      "*****\n",
      "Empty DataFrame\n",
      "Columns: [ID number, Assessment 1, Assessment 2, Online quiz 1 , Online quiz 2, Online quiz 3, Online quiz 4, Group assignment 1, Group assignment 2, Assessment 3, Assessment 4, Group assignment 3,  Group evaluation, Final exam, Class No, groupname, Student category, Total.marks, Grade, Status]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [ID number, Assessment 1, Assessment 2, Online quiz 1 , Online quiz 2, Online quiz 3, Online quiz 4, Group assignment 1, Group assignment 2, Assessment 3, Assessment 4, Group assignment 3,  Group evaluation, Final exam, Class No, groupname, Student category, Total.marks, Grade, Status]\n",
      "Index: []\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID number</th>\n",
       "      <th>Assessment 1</th>\n",
       "      <th>Assessment 2</th>\n",
       "      <th>Online quiz 1</th>\n",
       "      <th>Online quiz 2</th>\n",
       "      <th>Online quiz 3</th>\n",
       "      <th>Online quiz 4</th>\n",
       "      <th>Group assignment 1</th>\n",
       "      <th>Group assignment 2</th>\n",
       "      <th>Assessment 3</th>\n",
       "      <th>Assessment 4</th>\n",
       "      <th>Group assignment 3</th>\n",
       "      <th>Group evaluation</th>\n",
       "      <th>Final exam</th>\n",
       "      <th>Class No</th>\n",
       "      <th>groupname</th>\n",
       "      <th>Student category</th>\n",
       "      <th>Total.marks</th>\n",
       "      <th>Grade</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54002768</td>\n",
       "      <td>17.21</td>\n",
       "      <td>24.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>18.92</td>\n",
       "      <td>26.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>69</td>\n",
       "      <td>9</td>\n",
       "      <td>cl9tg2</td>\n",
       "      <td>domestic</td>\n",
       "      <td>57.0576</td>\n",
       "      <td>P</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54005658</td>\n",
       "      <td>66.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.0</td>\n",
       "      <td>18.14</td>\n",
       "      <td>29.00</td>\n",
       "      <td>19.91</td>\n",
       "      <td>22.35</td>\n",
       "      <td>17.04</td>\n",
       "      <td>60.54</td>\n",
       "      <td>24.07</td>\n",
       "      <td>42.5</td>\n",
       "      <td>1.88</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>cl2tg8</td>\n",
       "      <td>domestic</td>\n",
       "      <td>70.2472</td>\n",
       "      <td>Cr</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54010438</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>43.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>cl3tg7</td>\n",
       "      <td>international</td>\n",
       "      <td>17.0968</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54011658</td>\n",
       "      <td>60.67</td>\n",
       "      <td>29.07</td>\n",
       "      <td>27.5</td>\n",
       "      <td>18.40</td>\n",
       "      <td>24.50</td>\n",
       "      <td>18.52</td>\n",
       "      <td>25.00</td>\n",
       "      <td>19.29</td>\n",
       "      <td>66.11</td>\n",
       "      <td>27.61</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.72</td>\n",
       "      <td>75</td>\n",
       "      <td>20</td>\n",
       "      <td>cl20tg2</td>\n",
       "      <td>domestic</td>\n",
       "      <td>80.3012</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54012789</td>\n",
       "      <td>37.33</td>\n",
       "      <td>29.01</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>19.50</td>\n",
       "      <td>19.10</td>\n",
       "      <td>27.75</td>\n",
       "      <td>17.82</td>\n",
       "      <td>55.61</td>\n",
       "      <td>19.21</td>\n",
       "      <td>42.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "      <td>cl19tg6</td>\n",
       "      <td>domestic</td>\n",
       "      <td>49.9216</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID number  Assessment 1  Assessment 2  Online quiz 1   Online quiz 2  \\\n",
       "0   54002768         17.21         24.19             0.0           0.00   \n",
       "1   54005658         66.50          0.00            22.0          18.14   \n",
       "2   54010438          0.00          0.00             0.0           0.00   \n",
       "3   54011658         60.67         29.07            27.5          18.40   \n",
       "4   54012789         37.33         29.01            11.0          20.00   \n",
       "\n",
       "   Online quiz 3  Online quiz 4  Group assignment 1  Group assignment 2  \\\n",
       "0          15.00          18.92               26.57                0.00   \n",
       "1          29.00          19.91               22.35               17.04   \n",
       "2           4.13           0.00               18.86                0.00   \n",
       "3          24.50          18.52               25.00               19.29   \n",
       "4          19.50          19.10               27.75               17.82   \n",
       "\n",
       "   Assessment 3  Assessment 4  Group assignment 3   Group evaluation  \\\n",
       "0          0.00          0.00                50.0               2.00   \n",
       "1         60.54         24.07                42.5               1.88   \n",
       "2         43.75          0.00                22.5               0.00   \n",
       "3         66.11         27.61                35.0               1.72   \n",
       "4         55.61         19.21                42.5               2.00   \n",
       "\n",
       "   Final exam  Class No groupname Student category  Total.marks Grade  Status  \n",
       "0          69         9    cl9tg2         domestic      57.0576     P       1  \n",
       "1          63         2    cl2tg8         domestic      70.2472    Cr       1  \n",
       "2          15         3    cl3tg7    international      17.0968     F       0  \n",
       "3          75        20   cl20tg2         domestic      80.3012     D       1  \n",
       "4          33        19   cl19tg6         domestic      49.9216     F       0  "
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset[\"Assessment 1\"].isnull().head())\n",
    "print('*****')\n",
    "print(dataset[dataset[\"Assessment 1\"].isnull()])\n",
    "print(dataset[dataset[\"Assessment 2\"].isnull()])\n",
    "dataset = dataset.replace(\"NA\",0)\n",
    "dataset = dataset.replace(\"np.nan\",0)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part c</h2>\n",
    "<p>\n",
    "First we calculate the overall marks for each student.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.721\n",
       "1    6.650\n",
       "2    0.000\n",
       "3    6.067\n",
       "4    3.733\n",
       "Name: Assessment 1, dtype: float64"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1_contrib = dataset[\"Assessment 1\"]*(100/70)*0.07\n",
    "a1_contrib.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2.419\n",
       "1    0.000\n",
       "2    0.000\n",
       "3    2.907\n",
       "4    2.901\n",
       "Name: Assessment 2, dtype: float64"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2_contrib = dataset[\"Assessment 2\"]*(100/30)*0.03\n",
    "a2_contrib.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.000\n",
       "1    6.054\n",
       "2    4.375\n",
       "3    6.611\n",
       "4    5.561\n",
       "Name: Assessment 3, dtype: float64"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a3_contrib = dataset[\"Assessment 3\"]*(100/70)*0.07\n",
    "a3_contrib.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.000\n",
       "1    2.407\n",
       "2    0.000\n",
       "3    2.761\n",
       "4    1.921\n",
       "Name: Assessment 4, dtype: float64"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a4_contrib = dataset[\"Assessment 4\"]*(100/30)*0.03\n",
    "a4_contrib.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.00\n",
       "1    2.20\n",
       "2    0.00\n",
       "3    2.75\n",
       "4    1.10\n",
       "Name: Online quiz 1 , dtype: float64"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o1_contrib = dataset[\"Online quiz 1 \"]*(100/30)*0.03\n",
    "o1_contrib.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.000\n",
       "1    1.814\n",
       "2    0.000\n",
       "3    1.840\n",
       "4    2.000\n",
       "Name: Online quiz 2, dtype: float64"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o2_contrib = dataset[\"Online quiz 2\"]*(100/20)*0.02\n",
    "o2_contrib.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.500\n",
       "1    2.900\n",
       "2    0.413\n",
       "3    2.450\n",
       "4    1.950\n",
       "Name: Online quiz 3, dtype: float64"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o3_contrib = dataset[\"Online quiz 3\"]*(100/30)*0.03\n",
    "o3_contrib.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.892\n",
       "1    1.991\n",
       "2    0.000\n",
       "3    1.852\n",
       "4    1.910\n",
       "Name: Online quiz 4, dtype: float64"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o4_contrib = dataset[\"Online quiz 4\"]*(100/20)*0.02\n",
    "o4_contrib.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, you've probably realised that these (above) could've been more simply calculated as $x=y/10$. But what's coded above makes things crystal-clear. Because, just when you don't expect it, things change...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2.1256\n",
       "1    1.7880\n",
       "2    1.5088\n",
       "3    2.0000\n",
       "4    2.2200\n",
       "Name: Group assignment 1, dtype: float64"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1_contrib = dataset[\"Group assignment 1\"]*(100/30)*0.024\n",
    "g1_contrib.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.0000\n",
       "1    1.3632\n",
       "2    0.0000\n",
       "3    1.5432\n",
       "4    1.4256\n",
       "Name: Group assignment 2, dtype: float64"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g2_contrib = dataset[\"Group assignment 2\"]*(100/20)*0.016\n",
    "g2_contrib.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4.0\n",
       "1    3.4\n",
       "2    1.8\n",
       "3    2.8\n",
       "4    3.4\n",
       "Name: Group assignment 3, dtype: float64"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g3_contrib = dataset[\"Group assignment 3\"]*(100/50)*0.04\n",
    "g3_contrib.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2.00\n",
       "1    1.88\n",
       "2    0.00\n",
       "3    1.72\n",
       "4    2.00\n",
       "Name:  Group evaluation, dtype: float64"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ge_contrib = dataset[\" Group evaluation\"]*(100/2)*0.02\n",
    "ge_contrib.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    41.4\n",
       "1    37.8\n",
       "2     9.0\n",
       "3    45.0\n",
       "4    19.8\n",
       "Name: Final exam, dtype: float64"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe_contrib = dataset[\"Final exam\"]*0.60\n",
    "fe_contrib.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can, finally, calcuate students' final marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    57.0576\n",
       "1    70.2472\n",
       "2    17.0968\n",
       "3    80.3012\n",
       "4    49.9216\n",
       "Name: Total.marks, dtype: float64"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Total.marks'] = a1_contrib+a2_contrib+a3_contrib+a4_contrib+\\\n",
    "o1_contrib+o2_contrib+o3_contrib+o4_contrib+\\\n",
    "g1_contrib+g2_contrib+g3_contrib+ge_contrib+\\\n",
    "fe_contrib\n",
    "dataset['Total.marks'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, in Iguan &amp; Segu&iacute;&nbsp;(2017) there's no proper boxplot example. I started out with <code>matheplotlib</code>, like <a href=\"https://matplotlib.org/gallery/statistics/boxplot_demo.html\">here</a>, but it seems just too complicated for the simple task we're trying to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-250-6822ec5265ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtreatments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtreatments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#fig, ax = plt.subplots()\n",
    "import numpy as np\n",
    "treatments = []\n",
    "pos = np.array(range(treatments))+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, an <a href=\"https://seaborn.pydata.org/generated/seaborn.boxplot.html\">internet search</a> suggested <a href=\"https://anaconda.org/anaconda/seaborn\">seaborn</a> as a simple solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need <code>plt.show()</code> from <code>matplotlib.pyplot</code> to display the graphic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAERCAYAAACD9ivUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nw4OIiCT7f9v5ZFfzkvCjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb179e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "ax = sns.boxplot(x='Student category',y='Total.marks',data=dataset)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part d</h2>\n",
    "<p>\n",
    "For each tutorial group, provide summary statistics. Put these summary stats into a data table, <code>stat.table</code>.\n",
    "</p>\n",
    "<p>\n",
    "As noted in our text (p.&nbsp;23), the Pandas <code>groupby</code> function will be <i>very</i> helpful here. To get the mean, variance and standard deviation for each of the groups required a bit of web searching, and <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.DataFrameGroupBy.agg.html\">this</a> ultimately popped up: the aggregate function, or <code>agg</code>. Other approaches are, of course, possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">Total.marks</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>var</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class No</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59.280813</td>\n",
       "      <td>242.710561</td>\n",
       "      <td>15.579171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60.671069</td>\n",
       "      <td>160.151821</td>\n",
       "      <td>12.655110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57.610713</td>\n",
       "      <td>273.495919</td>\n",
       "      <td>16.537712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56.220069</td>\n",
       "      <td>319.034109</td>\n",
       "      <td>17.861526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>57.918085</td>\n",
       "      <td>383.676678</td>\n",
       "      <td>19.587666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Total.marks                       \n",
       "                mean         var        std\n",
       "Class No                                   \n",
       "1          59.280813  242.710561  15.579171\n",
       "2          60.671069  160.151821  12.655110\n",
       "3          57.610713  273.495919  16.537712\n",
       "4          56.220069  319.034109  17.861526\n",
       "5          57.918085  383.676678  19.587666"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_table = dataset.groupby('Class No').agg({\"Total.marks\": ['mean','var', 'std']})\n",
    "stat_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part e</h2>\n",
    "<p>\n",
    "Award a grade to each student, thus producing the vector <code>grade</code>.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID number</th>\n",
       "      <th>Total.marks</th>\n",
       "      <th>Grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54002768</td>\n",
       "      <td>57.0576</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54005658</td>\n",
       "      <td>70.2472</td>\n",
       "      <td>Cr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54010438</td>\n",
       "      <td>17.0968</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54011658</td>\n",
       "      <td>80.3012</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54012789</td>\n",
       "      <td>49.9216</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID number  Total.marks Grade\n",
       "0   54002768      57.0576     P\n",
       "1   54005658      70.2472    Cr\n",
       "2   54010438      17.0968     F\n",
       "3   54011658      80.3012     D\n",
       "4   54012789      49.9216     F"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Grade = []\n",
    "for row in dataset['Total.marks']:\n",
    "  if row < 50:\n",
    "    Grade.append('F')\n",
    "  elif row < 65:\n",
    "    Grade.append('P')\n",
    "  elif row < 75:\n",
    "    Grade.append('Cr')\n",
    "  elif row < 85:\n",
    "    Grade.append('D')\n",
    "  else:\n",
    "    Grade.append('HD')\n",
    "dataset['Grade'] = Grade\n",
    "dataset[[\"ID number\", \"Total.marks\", \"Grade\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part f</h2>\n",
    "<p>\n",
    "Fit: $y=\\beta_0+\\beta_1x$, where $y$ is <code>Final.exam</code> and $x$ is <code>Assessment.1</code>.\n",
    "</p>\n",
    "<p>\n",
    "First, let's prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    69\n",
       "1    63\n",
       "2    15\n",
       "3    75\n",
       "4    33\n",
       "Name: Final exam, dtype: int64"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pf = dataset['Final exam']\n",
    "y_pf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    17.21\n",
       "1    66.50\n",
       "2     0.00\n",
       "3    60.67\n",
       "4    37.33\n",
       "Name: Assessment 1, dtype: float64"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pf = dataset['Assessment 1']\n",
    "x_pf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use <code>sklearn</code> to do the regression (Igual &amp; Segu&iacute;&nbsp;2013, p.&nbsp;102). (Sigh, but it's not going to work: see below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[ 17.21  66.5    0.    60.67  37.33  47.44  69.3   66.89  19.25   0.    64.56\n  70.     0.    69.61  42.78  67.67  49.58  68.6   51.04   0.     0.    68.44\n  27.13  69.42  70.    63.78  68.54   0.    53.08  49.    35.     0.    64.94\n   0.     0.    67.38  67.08  55.27  57.94  23.92  57.17  68.25  40.4\n  69.13  29.07  20.61  60.2    0.    42.78  64.17  68.25  66.11  45.5\n  66.89  53.67  38.35  65.72  56.39  45.5    0.    66.5    0.    47.06\n  12.06  69.61  67.67  43.56  29.56   0.    68.13  66.97  49.    30.72\n  63.78  45.89  66.5   66.97  66.5   63.7   42.    44.63  58.92  68.44\n  69.22  68.83  62.61  44.04  68.83   0.    32.38  49.29  70.    35.7\n  66.11  52.21  32.67  17.97  64.46  62.56  25.67  64.87  70.    42.    66.27\n   0.    62.22  57.75  43.17  67.67  68.83  59.11   0.    46.28  63.93\n   4.67  41.42  70.    38.73  35.     0.    65.83  65.72  29.4   69.61\n  48.42   0.    68.06  32.28   6.27   0.    58.33  66.89  51.63  69.22   0.\n  56.88   0.    69.13   0.    61.02  69.22  68.25  44.33  63.78  61.44  63.\n  63.29  66.5   30.72  36.75  66.5   53.2   45.5   69.42   0.    59.89\n  64.56  58.72  29.56  69.42  40.83  52.21   0.    62.71  56.23  62.53\n  19.44  68.06  16.1   57.17  42.    38.97  65.33  58.33  56.    68.06\n  64.17  66.11  54.44  33.83  43.94  36.56  65.63  24.97   0.    67.96\n  48.13  52.97  68.69   0.    62.61  60.08   9.33  69.3   65.33  59.5\n  55.61  32.08  50.56  70.    63.39  43.56  28.29  54.83  67.96  68.83  35.\n  37.92  50.75  32.2   61.83  68.25  56.23  54.44  68.83   5.44  47.6   63.\n  67.67  63.47  50.17  65.8   63.    62.5   51.63  45.11  16.72  42.39\n  68.83   0.    67.38  66.89  55.71  49.78  49.    34.42  66.89  60.67\n  48.77   0.    50.75  61.54  68.25  68.25  67.28  37.92  63.78  57.17\n  59.21  65.72  45.89  47.48  60.96  66.89  68.64  38.73  48.3   29.4\n  64.56  67.28  27.65  36.94  66.79  33.13  29.94  19.44  70.    65.57\n  40.6   55.22  69.3   64.94  66.79  65.72  56.    64.94  46.9    9.72\n  59.03   0.    31.11  42.88  68.83  13.61  49.73  31.97  43.4   56.88\n  35.78  62.61  67.28  30.92  51.33  62.22  67.67  68.44  33.44  42.26\n  57.17  23.33  30.72  62.22  60.67  66.89  33.54  62.22  46.08  48.27\n  19.98  30.33  63.29  68.06   0.    23.33  65.92  65.72  16.92   9.63\n  60.67  41.61  32.67  59.03  66.5   52.5   66.11  67.38  66.97  67.67\n  65.04  68.83  40.83  63.    36.28  68.83  69.42  49.    68.25  64.46\n  60.67   0.    39.67  50.94  66.5   65.8    0.    31.06  35.19  29.9\n  36.94  67.28  37.72  67.9   57.75  43.56  40.06  29.94   0.    51.33\n  69.71  58.1   45.5   38.5   50.46   0.     0.    32.96  51.72   0.    37.92\n  26.83  63.    34.77  36.56  35.23  66.89  36.87  49.    67.38   0.    66.89\n  64.56  65.72  52.06  68.83  40.06  60.67  17.35  48.07  69.42  60.67   0.\n  57.17  69.61   0.    68.44  15.56  43.94  48.22  68.06  54.44  65.92\n  54.44  55.22  58.33  56.58  25.08  69.07  21.    40.83  35.58  59.21\n  62.22  64.46  23.33  68.37  25.38   0.    24.5    0.    69.61  68.06\n  63.44  37.33  66.11   0.    59.21  56.47  68.83  33.83   0.    46.67\n  41.88  61.13  22.4   36.56  29.63  46.67  67.08  35.58  34.22  36.17   0.\n  69.61  70.    57.56  28.88  32.67   0.    47.13  66.11  33.83  20.22\n  57.17  64.17  69.22  57.46  37.33  60.67  51.33  55.65  66.5   69.42\n  40.83  66.11  18.2   46.93  48.42  60.08  28.78  68.06  26.83  32.67\n  69.13  54.44  31.11   0.    64.94  64.17  66.21   0.    10.62  58.33\n  29.56   0.    55.42  63.39  46.67  68.25  24.21  52.11  46.67  67.08\n  34.42  67.28  67.28  11.67  65.04  70.    64.56  61.95  60.67   0.    65.04\n  66.01  66.89  48.22  47.37  34.71  45.21  65.63  26.83  35.19  50.46\n  68.83  53.9   60.67  50.75  24.89  66.5   63.    63.23  50.17  66.5\n  37.19   0.    69.13  60.08  38.21  42.29  44.8    7.39  64.46  61.83\n  51.33  10.6   59.97  47.25  31.11  43.4   70.    69.71  67.38  26.37\n  10.5   59.27  68.83  49.78  36.17  61.54   8.17  30.45  34.42  51.8\n  55.22  64.4   33.83  38.38  52.21  12.44  22.46  57.17  70.    62.85\n  61.69  62.22  36.02  66.5   25.9   27.07  68.25  21.88  62.27  63.    47.06\n  67.38   0.    28.35   0.  ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-256-2b835a69819e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfit_intercept\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_pf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\mq20069750\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    480\u001b[0m         \u001b[0mn_jobs_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[1;32m--> 482\u001b[1;33m                          y_numeric=True, multi_output=True)\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\mq20069750\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    540\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[0;32m    541\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[0;32m    543\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32mC:\\Users\\mq20069750\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    408\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 410\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    411\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m             \u001b[1;31m# To ensure that array flags are maintained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[ 17.21  66.5    0.    60.67  37.33  47.44  69.3   66.89  19.25   0.    64.56\n  70.     0.    69.61  42.78  67.67  49.58  68.6   51.04   0.     0.    68.44\n  27.13  69.42  70.    63.78  68.54   0.    53.08  49.    35.     0.    64.94\n   0.     0.    67.38  67.08  55.27  57.94  23.92  57.17  68.25  40.4\n  69.13  29.07  20.61  60.2    0.    42.78  64.17  68.25  66.11  45.5\n  66.89  53.67  38.35  65.72  56.39  45.5    0.    66.5    0.    47.06\n  12.06  69.61  67.67  43.56  29.56   0.    68.13  66.97  49.    30.72\n  63.78  45.89  66.5   66.97  66.5   63.7   42.    44.63  58.92  68.44\n  69.22  68.83  62.61  44.04  68.83   0.    32.38  49.29  70.    35.7\n  66.11  52.21  32.67  17.97  64.46  62.56  25.67  64.87  70.    42.    66.27\n   0.    62.22  57.75  43.17  67.67  68.83  59.11   0.    46.28  63.93\n   4.67  41.42  70.    38.73  35.     0.    65.83  65.72  29.4   69.61\n  48.42   0.    68.06  32.28   6.27   0.    58.33  66.89  51.63  69.22   0.\n  56.88   0.    69.13   0.    61.02  69.22  68.25  44.33  63.78  61.44  63.\n  63.29  66.5   30.72  36.75  66.5   53.2   45.5   69.42   0.    59.89\n  64.56  58.72  29.56  69.42  40.83  52.21   0.    62.71  56.23  62.53\n  19.44  68.06  16.1   57.17  42.    38.97  65.33  58.33  56.    68.06\n  64.17  66.11  54.44  33.83  43.94  36.56  65.63  24.97   0.    67.96\n  48.13  52.97  68.69   0.    62.61  60.08   9.33  69.3   65.33  59.5\n  55.61  32.08  50.56  70.    63.39  43.56  28.29  54.83  67.96  68.83  35.\n  37.92  50.75  32.2   61.83  68.25  56.23  54.44  68.83   5.44  47.6   63.\n  67.67  63.47  50.17  65.8   63.    62.5   51.63  45.11  16.72  42.39\n  68.83   0.    67.38  66.89  55.71  49.78  49.    34.42  66.89  60.67\n  48.77   0.    50.75  61.54  68.25  68.25  67.28  37.92  63.78  57.17\n  59.21  65.72  45.89  47.48  60.96  66.89  68.64  38.73  48.3   29.4\n  64.56  67.28  27.65  36.94  66.79  33.13  29.94  19.44  70.    65.57\n  40.6   55.22  69.3   64.94  66.79  65.72  56.    64.94  46.9    9.72\n  59.03   0.    31.11  42.88  68.83  13.61  49.73  31.97  43.4   56.88\n  35.78  62.61  67.28  30.92  51.33  62.22  67.67  68.44  33.44  42.26\n  57.17  23.33  30.72  62.22  60.67  66.89  33.54  62.22  46.08  48.27\n  19.98  30.33  63.29  68.06   0.    23.33  65.92  65.72  16.92   9.63\n  60.67  41.61  32.67  59.03  66.5   52.5   66.11  67.38  66.97  67.67\n  65.04  68.83  40.83  63.    36.28  68.83  69.42  49.    68.25  64.46\n  60.67   0.    39.67  50.94  66.5   65.8    0.    31.06  35.19  29.9\n  36.94  67.28  37.72  67.9   57.75  43.56  40.06  29.94   0.    51.33\n  69.71  58.1   45.5   38.5   50.46   0.     0.    32.96  51.72   0.    37.92\n  26.83  63.    34.77  36.56  35.23  66.89  36.87  49.    67.38   0.    66.89\n  64.56  65.72  52.06  68.83  40.06  60.67  17.35  48.07  69.42  60.67   0.\n  57.17  69.61   0.    68.44  15.56  43.94  48.22  68.06  54.44  65.92\n  54.44  55.22  58.33  56.58  25.08  69.07  21.    40.83  35.58  59.21\n  62.22  64.46  23.33  68.37  25.38   0.    24.5    0.    69.61  68.06\n  63.44  37.33  66.11   0.    59.21  56.47  68.83  33.83   0.    46.67\n  41.88  61.13  22.4   36.56  29.63  46.67  67.08  35.58  34.22  36.17   0.\n  69.61  70.    57.56  28.88  32.67   0.    47.13  66.11  33.83  20.22\n  57.17  64.17  69.22  57.46  37.33  60.67  51.33  55.65  66.5   69.42\n  40.83  66.11  18.2   46.93  48.42  60.08  28.78  68.06  26.83  32.67\n  69.13  54.44  31.11   0.    64.94  64.17  66.21   0.    10.62  58.33\n  29.56   0.    55.42  63.39  46.67  68.25  24.21  52.11  46.67  67.08\n  34.42  67.28  67.28  11.67  65.04  70.    64.56  61.95  60.67   0.    65.04\n  66.01  66.89  48.22  47.37  34.71  45.21  65.63  26.83  35.19  50.46\n  68.83  53.9   60.67  50.75  24.89  66.5   63.    63.23  50.17  66.5\n  37.19   0.    69.13  60.08  38.21  42.29  44.8    7.39  64.46  61.83\n  51.33  10.6   59.97  47.25  31.11  43.4   70.    69.71  67.38  26.37\n  10.5   59.27  68.83  49.78  36.17  61.54   8.17  30.45  34.42  51.8\n  55.22  64.4   33.83  38.38  52.21  12.44  22.46  57.17  70.    62.85\n  61.69  62.22  36.02  66.5   25.9   27.07  68.25  21.88  62.27  63.    47.06\n  67.38   0.    28.35   0.  ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "est = LinearRegression(fit_intercept = True)\n",
    "est.fit(x_pf, y_pf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above failed for these reasons <a href=\"#20180525_01\">[link]</a>. So, we fix it and proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(578, 1)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pf = x_pf[:, np.newaxis]\n",
    "x_pf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "est = LinearRegression(fit_intercept = True)\n",
    "est.fit(x_pf, y_pf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text also gives us commands for printing out the coefficients of the model (well, we have to adapt them to Python 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [ 0.27686542]\n",
      "Intercept: 37.0585815751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.096537361749078468"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Coefficients:\", est.coef_)\n",
    "print(\"Intercept:\", est.intercept_)\n",
    "est.score(x_pf, y_pf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmmm.... This isn't a lot of output. Let's look for something <font color=\"orange\">richer</font>. An <a href=\"https://medium.freecodecamp.org/data-science-with-python-8-ways-to-do-linear-regression-and-measure-their-speed-b5577d75f8b\">internet search</a> suggests the following. Note line 2: we have to add a constant manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(578, 2)\n",
      "[[  1.    17.21]\n",
      " [  1.    66.5 ]\n",
      " [  1.     0.  ]\n",
      " [  1.    60.67]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>Final exam</td>    <th>  R-squared:         </th> <td>   0.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   61.55</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 25 May 2018</td> <th>  Prob (F-statistic):</th> <td>2.11e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:36:02</td>     <th>  Log-Likelihood:    </th> <td> -2498.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   578</td>      <th>  AIC:               </th> <td>   5001.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   576</td>      <th>  BIC:               </th> <td>   5010.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   37.0586</td> <td>    1.824</td> <td>   20.320</td> <td> 0.000</td> <td>   33.477</td> <td>   40.641</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.2769</td> <td>    0.035</td> <td>    7.845</td> <td> 0.000</td> <td>    0.208</td> <td>    0.346</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 2.182</td> <th>  Durbin-Watson:     </th> <td>   1.887</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.336</td> <th>  Jarque-Bera (JB):  </th> <td>   2.245</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.146</td> <th>  Prob(JB):          </th> <td>   0.326</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.908</td> <th>  Cond. No.          </th> <td>    124.</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:             Final exam   R-squared:                       0.097\n",
       "Model:                            OLS   Adj. R-squared:                  0.095\n",
       "Method:                 Least Squares   F-statistic:                     61.55\n",
       "Date:                Fri, 25 May 2018   Prob (F-statistic):           2.11e-14\n",
       "Time:                        13:36:02   Log-Likelihood:                -2498.6\n",
       "No. Observations:                 578   AIC:                             5001.\n",
       "Df Residuals:                     576   BIC:                             5010.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         37.0586      1.824     20.320      0.000      33.477      40.641\n",
       "x1             0.2769      0.035      7.845      0.000       0.208       0.346\n",
       "==============================================================================\n",
       "Omnibus:                        2.182   Durbin-Watson:                   1.887\n",
       "Prob(Omnibus):                  0.336   Jarque-Bera (JB):                2.245\n",
       "Skew:                           0.146   Prob(JB):                        0.326\n",
       "Kurtosis:                       2.908   Cond. No.                         124.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "x_pf1 = sm.add_constant(x_pf)\n",
    "print(x_pf1.shape)\n",
    "print(x_pf1[0:4,:])\n",
    "model = sm.OLS(y_pf, x_pf1).fit()\n",
    "predictions = model.predict(x_pf1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#x2713; That's better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the model without the coefficient is better (in terms of $R^2$)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>Final exam</td>    <th>  R-squared:         </th> <td>   0.801</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.801</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   2325.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 25 May 2018</td> <th>  Prob (F-statistic):</th> <td>1.46e-204</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:36:02</td>     <th>  Log-Likelihood:    </th> <td> -2654.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   578</td>      <th>  AIC:               </th> <td>   5312.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   577</td>      <th>  BIC:               </th> <td>   5316.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th> <td>    0.9287</td> <td>    0.019</td> <td>   48.222</td> <td> 0.000</td> <td>    0.891</td> <td>    0.967</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 5.659</td> <th>  Durbin-Watson:     </th> <td>   1.927</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.059</td> <th>  Jarque-Bera (JB):  </th> <td>   5.742</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.242</td> <th>  Prob(JB):          </th> <td>  0.0566</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.932</td> <th>  Cond. No.          </th> <td>    1.00</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:             Final exam   R-squared:                       0.801\n",
       "Model:                            OLS   Adj. R-squared:                  0.801\n",
       "Method:                 Least Squares   F-statistic:                     2325.\n",
       "Date:                Fri, 25 May 2018   Prob (F-statistic):          1.46e-204\n",
       "Time:                        13:36:02   Log-Likelihood:                -2654.8\n",
       "No. Observations:                 578   AIC:                             5312.\n",
       "Df Residuals:                     577   BIC:                             5316.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1             0.9287      0.019     48.222      0.000       0.891       0.967\n",
       "==============================================================================\n",
       "Omnibus:                        5.659   Durbin-Watson:                   1.927\n",
       "Prob(Omnibus):                  0.059   Jarque-Bera (JB):                5.742\n",
       "Skew:                           0.242   Prob(JB):                       0.0566\n",
       "Kurtosis:                       2.932   Cond. No.                         1.00\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sm.OLS(y_pf, x_pf).fit()\n",
    "predictions = model.predict(x_pf)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<h4>Comments</h4>\n",
    "<p>\n",
    "The output above indicates both $\\beta_0$ and $\\beta_1$ are highly significant. The overall $R^2$ of the model, however, is quite low (just under 10%).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part g</h2>\n",
    "<p>\n",
    "Fit $y=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\beta_3x_3+\\beta_4x_4$, where $y$ is <code>Final.exam</code> and $x_1$ is <code>Assessment.1</code>, $x_2$ is <code>Assessment.3</code>, $x_3$ is <code>Online.quiz.1</code> and $x_4$ is <code>Group.assignment.2</code>.\n",
    "</p>\n",
    "<p>\n",
    "First, we'll get our data. Note how much more forgiving the <code>loc</code> attribute is (lines 1 and 2): doing a raw select (lines 3 and 4) on the columns without the space after &ldquo;Online quiz 1&rdquo; will throw an error!\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(578, 4)\n",
      "(578, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>Final exam</td>    <th>  R-squared:         </th> <td>   0.169</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.163</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   29.05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 25 May 2018</td> <th>  Prob (F-statistic):</th> <td>5.21e-22</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:36:02</td>     <th>  Log-Likelihood:    </th> <td> -2474.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   578</td>      <th>  AIC:               </th> <td>   4959.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   573</td>      <th>  BIC:               </th> <td>   4981.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "           <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>              <td>   25.8300</td> <td>    2.377</td> <td>   10.866</td> <td> 0.000</td> <td>   21.161</td> <td>   30.499</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Assessment 1</th>       <td>    0.1571</td> <td>    0.039</td> <td>    4.042</td> <td> 0.000</td> <td>    0.081</td> <td>    0.233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Assessment 3</th>       <td>    0.2030</td> <td>    0.048</td> <td>    4.222</td> <td> 0.000</td> <td>    0.109</td> <td>    0.297</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Online quiz 1 </th>     <td>    0.1465</td> <td>    0.102</td> <td>    1.439</td> <td> 0.151</td> <td>   -0.053</td> <td>    0.346</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Group assignment 2</th> <td>    0.3466</td> <td>    0.103</td> <td>    3.377</td> <td> 0.001</td> <td>    0.145</td> <td>    0.548</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 4.648</td> <th>  Durbin-Watson:     </th> <td>   1.910</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.098</td> <th>  Jarque-Bera (JB):  </th> <td>   4.716</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.218</td> <th>  Prob(JB):          </th> <td>  0.0946</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.922</td> <th>  Cond. No.          </th> <td>    242.</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:             Final exam   R-squared:                       0.169\n",
       "Model:                            OLS   Adj. R-squared:                  0.163\n",
       "Method:                 Least Squares   F-statistic:                     29.05\n",
       "Date:                Fri, 25 May 2018   Prob (F-statistic):           5.21e-22\n",
       "Time:                        13:36:02   Log-Likelihood:                -2474.6\n",
       "No. Observations:                 578   AIC:                             4959.\n",
       "Df Residuals:                     573   BIC:                             4981.\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "======================================================================================\n",
       "                         coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------------\n",
       "const                 25.8300      2.377     10.866      0.000      21.161      30.499\n",
       "Assessment 1           0.1571      0.039      4.042      0.000       0.081       0.233\n",
       "Assessment 3           0.2030      0.048      4.222      0.000       0.109       0.297\n",
       "Online quiz 1          0.1465      0.102      1.439      0.151      -0.053       0.346\n",
       "Group assignment 2     0.3466      0.103      3.377      0.001       0.145       0.548\n",
       "==============================================================================\n",
       "Omnibus:                        4.648   Durbin-Watson:                   1.910\n",
       "Prob(Omnibus):                  0.098   Jarque-Bera (JB):                4.716\n",
       "Skew:                           0.218   Prob(JB):                       0.0946\n",
       "Kurtosis:                       2.922   Cond. No.                         242.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pg = dataset.loc[:,[\"Assessment 1\", \"Assessment 3\", \"Online quiz 1\", \"Group assignment 2\"]]\n",
    "print(x_pg.shape)\n",
    "x_pg = dataset[[\"Assessment 1\", \"Assessment 3\", \"Online quiz 1 \", \"Group assignment 2\"]]\n",
    "print(x_pg.shape)\n",
    "y_pg = dataset[\"Final exam\"]\n",
    "x_pg = sm.add_constant(x_pg)\n",
    "model = sm.OLS(y_pg, x_pg).fit()\n",
    "predictions = model.predict(x_pg)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part h</h2>\n",
    "<p>\n",
    "Create an indicator variable, <code>Status</code>, indicating whether a student has passed or not.\n",
    "</p>\n",
    "<p>\n",
    "Fit $\\quad\\ln(\\frac{y}{1-y})=\\beta_0+\\beta_1x\\quad$, where $y$ is <code>Status</code> and $x$ is <code>Assessment.3</code>, to the first one hundred students (our training data). \n",
    "</p>\n",
    "<p>\n",
    "Compute the confusion matrix.\n",
    "</p>\n",
    "<p>\n",
    "Here's the first part: create the indicator variable. We'll make it 1 for a pass, and zero otherwise.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID number</th>\n",
       "      <th>Assessment 1</th>\n",
       "      <th>Assessment 2</th>\n",
       "      <th>Online quiz 1</th>\n",
       "      <th>Online quiz 2</th>\n",
       "      <th>Online quiz 3</th>\n",
       "      <th>Online quiz 4</th>\n",
       "      <th>Group assignment 1</th>\n",
       "      <th>Group assignment 2</th>\n",
       "      <th>Assessment 3</th>\n",
       "      <th>Assessment 4</th>\n",
       "      <th>Group assignment 3</th>\n",
       "      <th>Group evaluation</th>\n",
       "      <th>Final exam</th>\n",
       "      <th>Class No</th>\n",
       "      <th>groupname</th>\n",
       "      <th>Student category</th>\n",
       "      <th>Total.marks</th>\n",
       "      <th>Grade</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54002768</td>\n",
       "      <td>17.21</td>\n",
       "      <td>24.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>18.92</td>\n",
       "      <td>26.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>69</td>\n",
       "      <td>9</td>\n",
       "      <td>cl9tg2</td>\n",
       "      <td>domestic</td>\n",
       "      <td>57.0576</td>\n",
       "      <td>P</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54005658</td>\n",
       "      <td>66.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.0</td>\n",
       "      <td>18.14</td>\n",
       "      <td>29.00</td>\n",
       "      <td>19.91</td>\n",
       "      <td>22.35</td>\n",
       "      <td>17.04</td>\n",
       "      <td>60.54</td>\n",
       "      <td>24.07</td>\n",
       "      <td>42.5</td>\n",
       "      <td>1.88</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>cl2tg8</td>\n",
       "      <td>domestic</td>\n",
       "      <td>70.2472</td>\n",
       "      <td>Cr</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54010438</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>43.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>cl3tg7</td>\n",
       "      <td>international</td>\n",
       "      <td>17.0968</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54011658</td>\n",
       "      <td>60.67</td>\n",
       "      <td>29.07</td>\n",
       "      <td>27.5</td>\n",
       "      <td>18.40</td>\n",
       "      <td>24.50</td>\n",
       "      <td>18.52</td>\n",
       "      <td>25.00</td>\n",
       "      <td>19.29</td>\n",
       "      <td>66.11</td>\n",
       "      <td>27.61</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.72</td>\n",
       "      <td>75</td>\n",
       "      <td>20</td>\n",
       "      <td>cl20tg2</td>\n",
       "      <td>domestic</td>\n",
       "      <td>80.3012</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54012789</td>\n",
       "      <td>37.33</td>\n",
       "      <td>29.01</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>19.50</td>\n",
       "      <td>19.10</td>\n",
       "      <td>27.75</td>\n",
       "      <td>17.82</td>\n",
       "      <td>55.61</td>\n",
       "      <td>19.21</td>\n",
       "      <td>42.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "      <td>cl19tg6</td>\n",
       "      <td>domestic</td>\n",
       "      <td>49.9216</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID number  Assessment 1  Assessment 2  Online quiz 1   Online quiz 2  \\\n",
       "0   54002768         17.21         24.19             0.0           0.00   \n",
       "1   54005658         66.50          0.00            22.0          18.14   \n",
       "2   54010438          0.00          0.00             0.0           0.00   \n",
       "3   54011658         60.67         29.07            27.5          18.40   \n",
       "4   54012789         37.33         29.01            11.0          20.00   \n",
       "\n",
       "   Online quiz 3  Online quiz 4  Group assignment 1  Group assignment 2  \\\n",
       "0          15.00          18.92               26.57                0.00   \n",
       "1          29.00          19.91               22.35               17.04   \n",
       "2           4.13           0.00               18.86                0.00   \n",
       "3          24.50          18.52               25.00               19.29   \n",
       "4          19.50          19.10               27.75               17.82   \n",
       "\n",
       "   Assessment 3  Assessment 4  Group assignment 3   Group evaluation  \\\n",
       "0          0.00          0.00                50.0               2.00   \n",
       "1         60.54         24.07                42.5               1.88   \n",
       "2         43.75          0.00                22.5               0.00   \n",
       "3         66.11         27.61                35.0               1.72   \n",
       "4         55.61         19.21                42.5               2.00   \n",
       "\n",
       "   Final exam  Class No groupname Student category  Total.marks Grade  Status  \n",
       "0          69         9    cl9tg2         domestic      57.0576     P       1  \n",
       "1          63         2    cl2tg8         domestic      70.2472    Cr       1  \n",
       "2          15         3    cl3tg7    international      17.0968     F       0  \n",
       "3          75        20   cl20tg2         domestic      80.3012     D       1  \n",
       "4          33        19   cl19tg6         domestic      49.9216     F       0  "
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Status = []\n",
    "for row in dataset['Grade']:\n",
    "    if row == 'F':\n",
    "        Status.append(0)\n",
    "    else:\n",
    "        Status.append(1)\n",
    "dataset['Status'] = Status\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is to fit the logistic regression. This is covered (from a Python point of view) in your Igual &amp; Segu&iacute;&nbsp;(2017) book, in the chapter on regression.\n",
    "<p>\n",
    "First, we need to select the first 100 students, using their completion status as $y$ and <code>Assessment.3</code> as $x$. To select by rows and columns, <code>pandas</code> offers us <code>loc</code> and <code>iloc</code>, <a href=\"http://www.datacarpentry.org/python-ecology-lesson/02-index-slice-subset/\">for example</a> (found by a search for &lsquo;python select rows columns&rsquo;). (The Igual &amp; Segu&iacute;&nbsp;(2017) book uses the attribute <code>ix</code>, but if you did that you'd see it is now deprecated, and we should use <code>loc</code> or <code>iloc</code>.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    0\n",
       "3    1\n",
       "4    0\n",
       "Name: Status, dtype: int64"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "training_y = dataset.loc[0:99, 'Status']\n",
    "training_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the $x$ values will be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.00\n",
       "1    60.54\n",
       "2    43.75\n",
       "3    66.11\n",
       "4    55.61\n",
       "Name: Assessment 3, dtype: float64"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_x = dataset.loc[0:99,'Assessment 3']\n",
    "training_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"20180525_01\"></a>The following won't work, because our $x$ variable has to be a column vector, whereas currently it's a 1d array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[  0.    60.54  43.75  66.11  55.61  48.22  63.78  66.35  31.06  15.56\n  61.06  50.6   18.67  52.89   0.    64.5   47.4   10.5   61.54   0.    60.28\n  61.25  21.93  67.67  58.72  62.22  60.28  62.56  39.67  52.21  36.94   0.\n  55.22  44.72  11.67  58.33  67.9   63.29  61.25   0.    47.13  58.1\n  55.43  59.89  45.89  19.25  57.94  46.43  46.28  59.27  61.83  52.97\n  52.5   58.33  54.83  39.44  64.26   0.    62.22  29.17  61.83  54.69\n  57.56   0.     0.    63.73  35.39  11.28  47.06  61.06  49.78   0.    44.04\n  63.    52.11  56.17  63.    61.83  65.63  28.88   0.    58.63  61.44\n  60.52  54.98  53.89  31.03  56.    29.17   0.    47.83  57.87  61.83\n  55.77  51.33  56.39  49.    56.    63.58  42.44].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-268-e6c914a1b123>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlogreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\mq20069750\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[1;32m-> 1216\u001b[1;33m                          order=\"C\")\n\u001b[0m\u001b[0;32m   1217\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\mq20069750\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    540\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[0;32m    541\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[0;32m    543\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32mC:\\Users\\mq20069750\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    408\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 410\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    411\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m             \u001b[1;31m# To ensure that array flags are maintained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[  0.    60.54  43.75  66.11  55.61  48.22  63.78  66.35  31.06  15.56\n  61.06  50.6   18.67  52.89   0.    64.5   47.4   10.5   61.54   0.    60.28\n  61.25  21.93  67.67  58.72  62.22  60.28  62.56  39.67  52.21  36.94   0.\n  55.22  44.72  11.67  58.33  67.9   63.29  61.25   0.    47.13  58.1\n  55.43  59.89  45.89  19.25  57.94  46.43  46.28  59.27  61.83  52.97\n  52.5   58.33  54.83  39.44  64.26   0.    62.22  29.17  61.83  54.69\n  57.56   0.     0.    63.73  35.39  11.28  47.06  61.06  49.78   0.    44.04\n  63.    52.11  56.17  63.    61.83  65.63  28.88   0.    58.63  61.44\n  60.52  54.98  53.89  31.03  56.    29.17   0.    47.83  57.87  61.83\n  55.77  51.33  56.39  49.    56.    63.58  42.44].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "logreg.fit(training_x, training_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can turn our 1d array into a column vector, using the <code>newaxis</code> function from <code>numpy</code>, and used in Igual &amp; Segu&iacute;&nbsp;(2017, p.&nbsp;112)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_x1 = training_x[:, np.newaxis]\n",
    "training_x1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the logistic regression function works. (Note that we leave $y$ as a 1d-array, and do <i>not</i> transform it into a column vector.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(training_x1, training_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the estimated coefficients, $\\beta_0$ and $\\beta_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04515084]])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.09413416])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see how good this training model is by taking it to our test data (the students in the rows under the first 100).\n",
    "<p>\n",
    "We need our test data first. Again, we have to transform $x$ from a tuple to a column vector.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(478, 1)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x=dataset.loc[100:,'Assessment 3']\n",
    "test_x=test_x[:, np.newaxis]\n",
    "test_y = dataset.loc[100:, 'Status']\n",
    "#test_x.head()\n",
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 50.75,  57.83,  58.72,  66.5 ])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x[0:4,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 50.75  57.83  58.72  66.5 ]\n"
     ]
    }
   ],
   "source": [
    "print(test_x[0:4,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100    1\n",
       "101    1\n",
       "102    1\n",
       "103    1\n",
       "104    1\n",
       "Name: Status, dtype: int64"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Igual &amp; Segu&iacute;&nbsp;(2017, p.&nbsp;112) indicates we can get predicted values from the logistic regression by using the attribute <code>predict</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_pass = logreg.predict(test_x)\n",
    "pred_pass[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And Python implementations of the confusion matrix were discussed in chapter 5 of Igual &amp; Segu&iacute;&nbsp;(2017, p.&nbsp;74)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 35,  14],\n",
       "       [ 90, 339]], dtype=int64)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.confusion_matrix(pred_pass, test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
